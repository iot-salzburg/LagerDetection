'''
This file contains the template for the card recognition exercise

Follow the instructions given in the exercise sheet and complete the missing
code sections

'''


import cv2
import numpy as np

# defines minimum matches necessary to detect card
MIN_MATCH_COUNT= 15

print(cv2.__version__)

# create video object: argument can be either the device index (usually 0) or the name of a video file.
cap = cv2.VideoCapture(0)

# FLANN (Fast Library for Approximate Nearest Neighbors) matcher and its parameters
FLANN_INDEX_KDTREE = 0
index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)
search_params = dict(checks=50)   # or pass empty dictionary
flann = cv2.FlannBasedMatcher(index_params, search_params)

# or alternatively brute force matcher
#bf = cv2.BFMatcher(...)

#Bilder laden
modelimg1 = cv2.imread('modelimg_front.png') #Vorderseite der Karte
modelimg2 = cv2.imread('modelimg_back.png') #Rückseite der Karte
#modelimg2 = cv2.imread('modelimg_another_picture.png') #Rückseite der Karte

# create sift detector object
gray = cv2.cvtColor(modelimg1, cv2.COLOR_BGR2GRAY)
gray_2 = cv2.cvtColor(modelimg2, cv2.COLOR_BGR2GRAY)

sift = cv2.xfeatures2d.SIFT_create()
kp_1, des_1 = sift.detectAndCompute(gray, None)
kp_2, des_2 = sift.detectAndCompute(gray_2, None)

img_1 = cv2.drawKeypoints(gray,kp_1, None)
img_2 = cv2.drawKeypoints(gray_2,kp_2, None)

cv2.imwrite('sift_keypoints_front.jpg', img_1)
cv2.imwrite('sift_keypoints_back.jpg', img_2)

# just an idea for building up a model database (if somebody wants to detect multiple objects)
model_db=[]
model_db.append([kp_1, des_1, 'card_front', modelimg1])
#model_db.append([kp_2, des_2, 'card_back', modelimg2])

while(True):
    for model in model_db:
        # capture frame-by-frame: cap.read() returns a bool (True/False)
        ret, frame = cap.read()

        # operations on the frame
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

        kp_3, des_3 = sift.detectAndCompute(gray, None)

        if des_3 is None:
            img3 = frame

        if len(model[0]) >=2 and len(kp_3) >=2:
            matches = flann.knnMatch(model[1], des_3, k=2)

            good = []
            for m, n in matches:
                # match descriptors from frame
                if m.distance < 0.7 * n.distance:
                    good.append(m)

            if len(good) > MIN_MATCH_COUNT:
                src_pts = np.float32([model[0][m.queryIdx].pt for m in good]).reshape(-1, 1, 2)
                dst_pts = np.float32([kp_3[m.trainIdx].pt for m in good]).reshape(-1, 1, 2)

                M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)

                matchesMask = mask.ravel().tolist()

                h, w, _ = model[3].shape
                pts = np.float32([[0, 0], [0, h - 1], [w - 1, h - 1], [w - 1, 0]]).reshape(-1, 1, 2)

                if M is None:
                    img3 = frame

                else:
                    dst = cv2.perspectiveTransform(pts, M)

                    img2 = cv2.polylines(frame, [np.int32(dst)], True, 255, 3, cv2.LINE_AA)

                    draw_params = dict(matchColor=(0, 255, 0),  # draw matches in green color
                                       singlePointColor=None,
                                       matchesMask=matchesMask,  # draw only inliers
                                       flags=2)

                    img3 = cv2.drawMatches(model[3], model[0], frame, kp_3, good, None, **draw_params)
                    img3 = img2

            else:
                #print("Not enough matches are found - %d/%d" % (len(good), MIN_MATCH_COUNT))
                matchesMask = None

                img3 = frame

        else:
            img3 = frame

        cv2.imshow('card', img3) #model[2]

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Before exiting the program, release the capture
cap.release()
cv2.destroyAllWindows()
